{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementing the Actual Decision Tree and Prining the Information About each\n",
    "   Node \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Necessary Libraries for Creating a Decision Tree\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import math\n",
    "#This function is Used to convert data from being continuous to Discrete with classes \"1\",\"2\",\"3\",\"4\"\n",
    "def fun2(data,a1,a2,a3):\n",
    "    if(data<a1):\n",
    "        return \"1\"\n",
    "    if(data<a2):\n",
    "        return \"2\"\n",
    "    if(data<a3):\n",
    "        return \"3\"\n",
    "    else:\n",
    "        return \"4\"\n",
    "#This function is used to Convert data from continuous to descrete. This is done by placing the value on the bases of mean, the mean of min_value\n",
    "# and mean, mean of max_val and mean. This replaces the Value in Place.\n",
    "def fun1(data,column):\n",
    "    min_val=data[column].min()\n",
    "    n1=data[column].mean()\n",
    "    n2=(n1+min_val)/2\n",
    "    max_val=data[column].max()\n",
    "    n3=(max_val+n1)/2\n",
    "    arr=[]\n",
    "    arr.append(n2)\n",
    "    arr.append(n1)\n",
    "    arr.append(n3)\n",
    "    data[column]=data[column].apply(fun2,args=arr)\n",
    "    \n",
    "#Loading the iris data from datasets in sklearn    \n",
    "iris=load_iris()\n",
    "x_iris=iris.data\n",
    "#taking the x value or features from the data\n",
    "y_iris=iris.target\n",
    "#taking the y value or output from the data\n",
    "x_data=pd.DataFrame(x_iris)\n",
    "#Renaming the columns for convinience\n",
    "x_data.columns=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "#Making the data in each column descrete\n",
    "fun1(x_data,\"sepal_length\")\n",
    "fun1(x_data,\"sepal_width\")\n",
    "fun1(x_data,\"petal_length\")\n",
    "fun1(x_data,\"petal_width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,split_data,out,level):\n",
    "        self.split_data=split_data\n",
    "        #This is used to store the data of the split_feature that has be used to split the Decision Tree here. It is defined as Null\n",
    "        #or None for Leaf Node\n",
    "        self.children={}\n",
    "        #The children represents the Node that are connected to the particular Node and This has key with the feature_value that\n",
    "        #it has been split upon and the value as the child Node\n",
    "        self.level=level\n",
    "        #This is used to store the level of the data which is present upon \n",
    "        self.out=out\n",
    "        #This helps to store the output that will be predicted for this particular node\n",
    "        self.entropy=0.0\n",
    "        #Entropy for the Particular Node\n",
    "        self.gainRatio=-1\n",
    "        #Gain Ratio for that Node\n",
    "        \n",
    "    def add(self,feature_val,object):\n",
    "        self.children[feature_val]=object\n",
    "        #This is Used to add the child node based on the feature value to a node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Optimum Functions for Calculating the split feature\n",
    "\n",
    "#This function is used to Calculate the Frequency of the particular class in the output dataset\n",
    "def freq(y):\n",
    "    d1={}    #Dictionary to store the Frequency of each class\n",
    "    for x in y: \n",
    "        if(x in d1):\n",
    "            d1[x]=d1[x]+1\n",
    "        else:\n",
    "            d1[x]=1\n",
    "    return d1\n",
    "#Return the dictionary with key as the classes present in the data and the count of each class as the value\n",
    "\n",
    "#This Function is used to calculate the entropy of the data \n",
    "def entropy(y):\n",
    "    d1=freq(y) #Calculating Frequency of each class in the data\n",
    "    size=len(y) # \n",
    "    ent=0\n",
    "    for x in d1:\n",
    "        test=d1[x]/size\n",
    "        ent+=(-1*(test))*math.log2(test)\n",
    "    return ent\n",
    "\n",
    "#This is used to calculate the gain Ratio which will be further used in splitting the data as gain ratio is the deciding factor for spliitng the data\n",
    "def gainRatio(x,y,feature):\n",
    "    ent1=entropy(y) #Defining the Orignal entropy before split\n",
    "    ent2=0 #Initialising the entropy for split data\n",
    "    values=set(x[:,feature])\n",
    "    split=0 #initialising the split ratio\n",
    "    frame=pd.DataFrame(x) \n",
    "    frame[frame.shape[1]]=y #Adding the Result Value to the data\n",
    "    size=frame.shape[0]\n",
    "    #Running the loop for calculating info for each of the classes in the split feature i.e feature\n",
    "    for x in values:\n",
    "        datax=frame[frame[feature]==x] #Getting the data with particular class\n",
    "        size1=datax.shape[0]\n",
    "        ent2+=(size1/size)*entropy(datax[datax.shape[1]-1]) #Calculating the Information for that particular Class\n",
    "        split+=(-size1/size)*math.log2(size1/size) #Calculating the split_info for that class\n",
    "    gain=ent1-ent2 #Calculating the Information Gain\n",
    "    ratio=gain/split #Calculating the Split Ratio\n",
    "    return ratio #Returns the Gain Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing functions for Prediction of data\n",
    "def predict(x,root):\n",
    "    y=[]\n",
    "    #Loop for prediction of data\n",
    "    for i in range(len(x)):\n",
    "        y.append(0) #Appending default value zero to data\n",
    "        y[i]=pred(x[i],root) #Predicting data for each point\n",
    "    return y #Returning the predicting y\n",
    "def pred(xpoint,node):\n",
    "    if(len(node.children)==0): #If len of children is zero or reached leaf node\n",
    "        return node.out #Returning the predicted output for that node\n",
    "    \n",
    "    feature=xpoint[node.split_data] #Getting the split feature of the Node\n",
    "    if(feature in node.children): #If class present in the child Node then recursively\n",
    "        #calling the function with childNode\n",
    "        return pred(xpoint,node.children[feature])\n",
    "    else: #Else returning the output for that Node\n",
    "        return node.out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of  0 = 36\n",
      "Count of  1 = 40\n",
      "Count of  2 = 36\n",
      "Current Entropy is = 1.583143101527774\n",
      "Splitting on feature petal_width With Gain Ratio 0.7068646591932941\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 25\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 7\n",
      "Count of 2 = 0\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 36\n",
      "Count of 1 = 0\n",
      "Count of 2 = 0\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of  0 = 0\n",
      "Count of  1 = 33\n",
      "Count of  2 = 11\n",
      "Current Entropy is = 0.8112781244591328\n",
      "Splitting on feature petal_length With Gain Ratio 0.45551250909754704\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 6\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 1\n",
      "Count of 2 = 0\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of  0 = 0\n",
      "Count of  1 = 32\n",
      "Count of  2 = 5\n",
      "Current Entropy is = 0.5713549744279549\n",
      "Splitting on feature sepal_length With Gain Ratio 0.12275198489935117\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 2\n",
      "Count of 2 = 0\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 12\n",
      "Count of 2 = 0\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of  0 = 0\n",
      "Count of  1 = 18\n",
      "Count of  2 = 4\n",
      "Current Entropy is = 0.6840384356390417\n",
      "Splitting on feature sepal_width With Gain Ratio 0.10426011804142046\n",
      "\n",
      "Level 4\n",
      "Count of  0 = 0\n",
      "Count of 1 = 10\n",
      "Count of 2 = 4\n",
      "Current Entropy is = 0.863120568566631\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of  0 = 0\n",
      "Count of 1 = 3\n",
      "Count of  2 = 0\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of  0 = 0\n",
      "Count of 1 = 5\n",
      "Count of  2 = 0\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "35 38\n",
      "The Score for the Data is 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "#Implementation for Formation of Decision Tree\n",
    "\n",
    "def decisionTree(x,y,features,classes,level):\n",
    "    if(len(features)==0): #First Base Condition When No Features are left that can be Used to Split the Decision Tree\n",
    "        out=\"\" #Output the Node will Predict\n",
    "        max_count=-1 #Initialising the Count for the freq of class in data as Majority class will be the output\n",
    "        freqs=freq(y) #Calculating the Frequency of different classed in target\n",
    "        print(\"Level\",level) #Printing the Current Level Of the Tree \n",
    "        #This loop is used to print the count of classes in target and used to select the one with the max count which will be \n",
    "        #Output\n",
    "        for i in classes:\n",
    "            if(not(i in freqs)):\n",
    "                print(\"Count of \",i,\"=\",0) \n",
    "            else:\n",
    "                if(freqs[i]>max_count):\n",
    "                    max_count=freqs[i]\n",
    "                    out=i #The Out Value for this node\n",
    "                print(\"Count of\",i,\"=\",freqs[i])\n",
    "        print(\"Current Entropy is =\",entropy(y)) #Printing the Entropy for current Node\n",
    "        print(\"Reached Leaf Node\") #Print the ent of tree or leaf Node\n",
    "        print()\n",
    "        t1=Node(None,out,level) #return Node with level and ouput\n",
    "        t1.entropy=entropy(y) #storing the entropy in the Node\n",
    "        return t1 #Returning the Node\n",
    "    \n",
    "    if(len(set(y))==1):\n",
    "        print(\"Level\",level) #Printing the Level of the Node\n",
    "        out=\"\" #The ouput for the Node\n",
    "        #This loop is used to print the count of classes in target and used to select the class that is the only class in target \n",
    "        for i in classes:\n",
    "            if(i in y):\n",
    "                out=i #Out Value for this Node\n",
    "                print(\"Count of\",i,\"=\",len(y))\n",
    "            else:\n",
    "                print(\"Count of\",i,\"=\",0)\n",
    "                \n",
    "        print(\"Current Entropy is = 0.0\") #This is the Entropy for the Current Node\n",
    "        print(\"Reached Leaf Node\") #Printing that the Tree has reached the end or leaf Node\n",
    "        print()\n",
    "        t1=Node(None,out,level) # Creating the leaf Node \n",
    "        return t1 #Returning the Node\n",
    "    \n",
    "    gain=-9999999999 #Initialising the Gain with some value\n",
    "    feature=None #Feature that will be used to split\n",
    "    #This loop traverses through the features and calculates the Gain for each of them and compares. The Feature which results\n",
    "    #Maximum Gain ratio is selected and feature represents the splitting feature\n",
    "    for i in features:\n",
    "        cgain=gainRatio(x,y,i)\n",
    "        if(cgain>gain):\n",
    "            gain=cgain\n",
    "            feature=i\n",
    "    print(\"Level\",level) #Printing the Level of the Node in the Tree\n",
    "    freqs=freq(y) #Calculating the Frequency For each class in target data\n",
    "    count=-1 \n",
    "    out=\"\"\n",
    "    #This loop is used to print the count of classes in target and used to select the one with the max count which will be \n",
    "    #Output\n",
    "    for i in classes:\n",
    "        if(i in freqs):\n",
    "            print(\"Count of \",i,\"=\",freqs[i])\n",
    "            if(freqs[i]>count):\n",
    "                count=freqs[i]\n",
    "                out=i\n",
    "        else:\n",
    "            print(\"Count of \",i,\"= 0\")\n",
    "    print(\"Current Entropy is =\",entropy(y)) #Entropy for the Current Node\n",
    "    print(\"Splitting on feature\",x_data.columns[feature],\"With Gain Ratio\",gain) #This Prints the feature on which the data \n",
    "    #is split\n",
    "    print(\"\")\n",
    "    \n",
    "    split_class=set(x[:,feature])#Getting the Various Classes in the feature which will be used to split the data\n",
    "    frame=pd.DataFrame(x) #Creating a DataFrame for the numpy data\n",
    "    frame[frame.shape[1]]=y #Joing the output to the data\n",
    "    c_Node=Node(feature,out,level) #Creating a Node with the data, output precited for that Node and the level\n",
    "    c_Node.entropy=entropy(y) #Assigning the Entropy value to the Node\n",
    "    c_Node.gainRatio=gainRatio(x,y,feature) # Assigning the Gain Ratio to the Node\n",
    "    features.remove(feature) #Removing the Feature that was used to split the data as the a feature can be only used once\n",
    "    #Running a for loop for classes in the feature which was used to split the data so as to sort the data based on that class\n",
    "    # and Recursively call the Decision Tree Function.\n",
    "    for i in split_class:\n",
    "        n_frame=frame[frame[feature]==i] #Data with the Particular class\n",
    "        x_new=n_frame.iloc[:,0:n_frame.shape[1]-1].values #The x data for the recursive call with only selected class from feature\n",
    "        y_new=n_frame.iloc[:,n_frame.shape[1]-1].values #The y data for the same\n",
    "        ch_Node=decisionTree(x_new,y_new,features,classes,level+1) \n",
    "        #Recursively calling the function with incremented Level\n",
    "        #and new x and y and feature arra\n",
    "        c_Node.add(i,ch_Node) #Adding this Node to the child of data\n",
    "    \n",
    "    return c_Node #Returning the Node \n",
    "        \n",
    "#Function to start the Formation of Decision Tree\n",
    "def fit(x,y):\n",
    "    split_features=[] #Array that stores the indexes of feature in data \n",
    "    for i in range(len(x[0])):\n",
    "        split_features.append(i)\n",
    "    classes=set(y) #Classes Present in the Output data Here 0,1,2\n",
    "    root=decisionTree(x,y,split_features,classes,0) #Function to form Decision Tree \n",
    "    return root #Returns the Root of the Tree.\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_data.values,y_iris,random_state=100)\n",
    "root=fit(x_train,y_train)\n",
    "y=predict(x_test,root)\n",
    "count=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i]==y[i]):\n",
    "        count=count+1\n",
    "print(count,len(y_test))\n",
    "print(\"The Score for the Data is\",count/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
